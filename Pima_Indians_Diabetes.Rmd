---
title: "Pima Indians Diabetes 数据分析与 GLM 模型训练"
author: ""
date: "`r Sys.Date()`"
output: html_document
---


## 1. Data loading

```{r chunk1,eval=FALSE}
# If the mlbench package is not installed yet, please install it first

library(mlbench)


# load
data("PimaIndiansDiabetes")

df <- PimaIndiansDiabetes
str(df)
```
```{r chunk1.1}
head(df)
```






> **Description**
> The dataset contains 768 rows and 9 columns, and the main attributes include:
>
> * **pregnant**: number of pregnancies
> * **glucose**: plasma glucose concentration
> * **pressure**: diastolic blood pressure (mm Hg)
> * **triceps**: triceps skinfold thickness (mm)
> * **insulin**: 2-hour serum insulin (mu U/ml)
> * **mass**: BMI (body mass index)
> * **pedigree**: diabetes family history function
> * **age**: age
> * **diabetes**: target variable, factor category ("pos" means diabetes, "neg" means no disease)


## 2. cleaning and data preparing

### 2.1 NA

```{r chunk2}
# View basic statistics for each variable
summary(df)

# Check the number of missing values
sapply(df, function(x) sum(is.na(x)))
```


### 2.2 Dealing with outliers

A simple check on the glucose variable

```{r handle-zeros}
# Check the number of 0 values in glucose
sum(df$glucose == 0)

# Cannot be 0
df$glucose <- ifelse(df$glucose == 0, NA, df$glucose)

# View the updated missing values
table(is.na(df$glucose))
```
```{r handle-zeros.1}
# the mean of glucose
glucose_mean <- mean(df$glucose, na.rm = TRUE)

# substitute
df$glucose[is.na(df$glucose)] <- glucose_mean
```

```{r handle-zeros-rest}
# List of variables to clean (where 0 is likely invalid)
vars_to_clean <- c("pressure", "triceps", "insulin", "mass")

# Loop over variables
for (var in vars_to_clean) {
  # Convert 0 to NA
  df[[var]] <- ifelse(df[[var]] == 0, NA, df[[var]])
  
  # Compute mean without NA
  var_mean <- mean(df[[var]], na.rm = TRUE)
  
  # Replace NA with mean
  df[[var]][is.na(df[[var]])] <- var_mean
}
```

### 2.3 preprocessing

```{r preprocessing}
# For the target variable diabetes, confirm that it is a factor (if it is not already a factor)
df$diabetes <- as.factor(df$diabetes)

```


## 3.  Feature Interaction Analysis

### 3.1  Distribution of Numerical Features

```{r univariate-plots, fig.width=7, fig.height=5}
library(ggplot2)

# Plot histogram of age
ggplot(df, aes(x = age)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "black") +
  labs(title = "Distribution of Age", x = "Age", y = "Count")

# Plot histogram of glucose
ggplot(df, aes(x = glucose)) +
  geom_histogram(bins = 30, fill = "lightgreen", color = "black") +
  labs(title = "Distribution of Glucose", x = "Plasma Glucose Concentration", y = "Count")

```

### 3.2 Categorical Feature Impact on Target

```{r categorical-analysis}
# Frequency of different pregnancy counts
table(df$pregnant)

# Analyze the effect of number of pregnancies on diabetes outcome
ggplot(df, aes(x = as.factor(pregnant), fill = diabetes)) +
  geom_bar(position = "fill") +
  labs(title = "Proportion of Diabetes by Pregnancy Count", 
       x = "Number of Pregnancies", 
       y = "Proportion", 
       fill = "Diabetes Status")

```
The sample size is obviously decreasing as the number of pregnancies increases.
It seems that the more pregnancies you have, the higher your risk of diabetes, but the data is not supportive at this time.

```{r p-analysis}
model <- glm(diabetes ~ pregnant, data = df, family = binomial)
summary(model)
```
Although the sample size is small at very high pregnancies (e.g., ≥14), and the proportion graph may exaggerate the trend, overall, the logistic regression shows that the number of pregnancies is significantly positively associated with the incidence of diabetes. For each additional pregnancy, the odds of diabetes increase by approximately 14.7%.



### 3.3 Cross-Feature Interaction: Scatter Plots
```{r test0}
sum(is.na(df$glucose))     
sum(is.na(df$age))         
```

```{r scatter-analysis}
# Scatter plot: age vs. glucose colored by diabetes status
ggplot(df, aes(x = age, y = glucose, color = diabetes)) +
  geom_point() +
  labs(title = "Age vs. Glucose Colored by Diabetes Status", 
       x = "Age", 
       y = "Plasma Glucose Concentration", 
       color = "Diabetes Status")

```
Among people with high blood sugar, the proportion of diabetes is higher, and the impact of age is not as significant as blood sugar




```{r boxplot1}
library(ggplot2)

# Boxplots for each variable
vars_to_plot <- c("pressure", "triceps", "insulin", "mass", "pedigree")

for (var in vars_to_plot) {
  p <- ggplot(train_data, aes_string(x = "factor(diabetes)", y = var)) +
    geom_boxplot(fill = "skyblue") +
    labs(title = paste("Boxplot of", var, "by Diabetes Status"),
         x = "Diabetes (0 = No, 1 = Yes)",
         y = var)
  print(p)
}


```

The reason for the numerous outliers in **serum insulin** may be that insulin levels in the human body naturally fluctuate significantly.

I believe this feature may not contribute positively to the model's performance. Therefore, it's important to compare the results of models with and without this feature during training.

Triceps skinfold thickness is a commonly used anthropometric method for estimating body fat percentage. However, I also believe that this feature does not contribute positively to the model's performance.


```{r Density Plot}
for (var in vars_to_plot) {
  p <- ggplot(train_data, aes_string(x = var, fill = "factor(diabetes)")) +
    geom_density(alpha = 0.4) +
    labs(title = paste("Density Plot of", var, "by Diabetes Status"),
         x = var, fill = "Diabetes") +
    theme_minimal()
  print(p)
}

```
Pressure: The overall blood pressure of diabetic patients is slightly higher. The difference is not significant.

Triceps: The two categories almost completely overlap, and the peak position in the middle is close.

Insulin: The data is very biased, with dense spikes and a large number of outliers.

Mass: The overall BMI of diabetic patients is higher.

Pedigree: People with high pedigree values are more likely to be ill.

```{r Facet Histogram}
for (var in vars_to_plot) {
  p <- ggplot(train_data, aes_string(x = var)) +
    geom_histogram(bins = 30, fill = "lightgreen", color = "black") +
    facet_wrap(~diabetes) +
    labs(title = paste("Histogram of", var, "by Diabetes Status"),
         x = var, y = "Count") +
    theme_minimal()
  print(p)
}


```


```{r boxplot4}

```

```{r boxplot5}

```


















## 4. GLM Model Training



### 4.1 Train/Test Split

```{r train-test-split}

library(caret)


# Set seed for reproducibility
set.seed(123)

# Split dataset: 70% training, 30% testing
train_index <- createDataPartition(df$diabetes, p = 0.7, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]
```



### 4.2 Train the Logistic Regression Model (GLM with Binomial Family)

```{r glm-training}
# Train logistic regression using all available features
glm_model <- glm(diabetes ~ ., data = train_data, family = "binomial")

# View model summary
summary(glm_model)
```
Based on the model output above, the non-significant variables include pressure, triceps, and insulin. These variables have p-values greater than 0.05.

```{r glm-training}
# Train logistic regression without the non-significant variables
diabetes_glm <- glm(diabetes ~ pregnant + glucose + mass + pedigree + age, 
                    data = train_data, 
                    family = "binomial")

# View summary of the simplified model
summary(diabetes_glm)
```

### 4.3 Prediction and Model Evaluation

```{r glm-prediction}
# Predict probabilities on the test set
pred_prob <- predict(glm_model, newdata = test_data, type = "response")

# Convert probabilities to class labels using a 0.5 threshold
pred_class <- ifelse(pred_prob > 0.5, "pos", "neg")
pred_class <- factor(pred_class, levels = levels(test_data$diabetes))

# Build confusion matrix
conf_matrix <- table(Predicted = pred_class, Actual = test_data$diabetes)
print(conf_matrix)

# Calculate accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Accuracy =", accuracy, "\n")
```



###  4.4 Evaluation Metrics: Precision, Recall, F1 Score, ROC AUC

```{r glm-metrics}
# Load required libraries
library(caret)
library(pROC)

# Precision
precision <- posPredValue(pred_class, test_data$diabetes, positive = "pos")

# Recall (also called Sensitivity)
recall <- sensitivity(pred_class, test_data$diabetes, positive = "pos")

# F1 Score
f1_score <- 2 * precision * recall / (precision + recall)

# Print metrics
cat("Precision:", round(precision, 3), "\n")
cat("Recall:", round(recall, 3), "\n")
cat("F1 Score:", round(f1_score, 3), "\n")
```


### 4.5 ROC Curve and AUC (Area Under Curve)

```{r glm-roc-auc, message=FALSE}
# Convert true labels to numeric: pos = 1, neg = 0
actual_numeric <- ifelse(test_data$diabetes == "pos", 1, 0)

# Compute ROC
roc_obj <- roc(actual_numeric, pred_prob)

# Plot ROC Curve
plot(roc_obj, main = "ROC Curve for GLM", col = "blue")

# Calculate AUC
auc_value <- auc(roc_obj)
cat("AUC (Area Under the Curve):", round(auc_value, 3), "\n")
```

