---
title: "diabetes_prediction_dataset"
output: 
  pdf_document:
    latex_engine: xelatex
author: ""
date: "`r Sys.Date()`"
---

## Feature Description

### `gender`: Biological Sex  
- **male**: 41%  
- **female**: 59%  
- **other**: 0%  

Gender may influence an individual's susceptibility to diabetes.

---

### `age`: Age  
Age is an important risk factor for diabetes — generally, older individuals have a higher likelihood of developing it.  
The age range in this dataset is **0 to 80 years**.

---

### `hypertension`: Presence of Hypertension  
- `0`: No hypertension  
- `1`: Has hypertension  

Hypertension is a known comorbidity and risk factor for diabetes.

---

### `heart_disease`: Presence of Heart Disease  
- `0`: No heart disease  
- `1`: Has heart disease  

Heart disease is strongly associated with increased diabetes risk.

---

### `smoking_history`: Smoking Behavior  
Smoking is a risk factor for diabetes and can worsen its complications. Categories include:

- `not current`: Previously smoked but not currently  
- `former`: Former smoker, now quit  
- `No Info`: No information available  
- `current`: Currently smoking  
- `never`: Never smoked  
- `ever`: Smoked at some point in the past

---

### `bmi`: Body Mass Index  
BMI is calculated from height and weight to assess body fat. Range: **10.16 to 71.55**

- BMI < 18.5: Underweight  
- 18.5 ≤ BMI < 24.9: Normal  
- 25 ≤ BMI < 29.9: Overweight  
- BMI ≥ 30: Obese

---

### `HbA1c_level`: Hemoglobin A1c  
HbA1c reflects average blood glucose levels over the past **2–3 months**.  
Levels above **6.5%** typically indicate diabetes risk.

---

### `blood_glucose_level`: Blood Glucose Level  
This refers to the concentration of glucose in the blood at a specific time.  
Higher glucose levels are a key indicator of diabetes.

---

### `diabetes`: Target Variable (Diabetes Status)  
- `0`: Does not have diabetes  
- `1`: Has diabetes

Objective1:
Find out which features (such as age, bmi, gender, etc.) may have a significant statistical or pattern relationship with diabetes (values of 0 or 1)

---

Read the Diabetes prediction dataset and view the first five rows of the diabetes_prediction_dataset:


```{r chunk1}
diabetes_dataset = read.csv("./diabetes_prediction_dataset.csv")
head(diabetes_dataset, 5)
```
```{r chunk1.1}
# Check column names and whether they need to be renamed
colnames(diabetes_dataset)

# Replace spaces (if any)
# colnames(diabetes_dataset) <- gsub(" ", "_", colnames(diabetes_dataset))
```
Check if there are exact duplicate rows???
This dataset does not contain unique identifiers, so we do not check for duplicate rows, since each duplicate row is a failed entry.




View dataset information
```{r chunk2}
str(diabetes_dataset)
summary(diabetes_dataset)
```




Check the values of gender and smoking_history. Determine whether to convert to dummy variables (dummy/one-hot) or label encoding.
```{r chunk2.1}
# View all unique values for gender and smoking_history
unique(diabetes_dataset$gender)
unique(diabetes_dataset$smoking_history)

# View frequency distribution in detail
table(diabetes_dataset$gender)
table(diabetes_dataset$smoking_history)
```


Check if there are missing values in the dataset
```{r chunk3}
anyNA(diabetes_dataset)         # any missing values
colSums(is.na(diabetes_dataset))  # num of missing values in each column
```
This data set does not have missing values, so there is no need to fill missing values


```{r chunk2.2}
# convert gender to factor
diabetes_dataset$gender <- factor(diabetes_dataset$gender)

# dummy variables
gender_dummies <- model.matrix(~ gender - 1, data = diabetes_dataset)

# add it back
diabetes_dataset <- cbind(diabetes_dataset, gender_dummies)
head(diabetes_dataset, 5)
```
```{r chunk2.3}
library(dplyr)

# Merge smoking_history categories into more logical groups
diabetes_dataset$smoking_status_grouped <- dplyr::case_when(
  diabetes_dataset$smoking_history == "never" ~ "never",
  diabetes_dataset$smoking_history == "current" ~ "current",
  diabetes_dataset$smoking_history %in% c("former", "not current", "ever") ~ "former",
  TRUE ~ "unknown"
)

# convert gender to factor
diabetes_dataset$smoking_status_grouped <- factor(diabetes_dataset$smoking_status_grouped)

# one-hot encoding
smoking_dummies <- model.matrix(~ smoking_status_grouped - 1, data = diabetes_dataset)

# add it back
diabetes_dataset <- cbind(diabetes_dataset, smoking_dummies)
head(diabetes_dataset, 5)
```


Data Cleaning and Outlier Handling
```{r chunk4}
# Select continuous variables
num_vars <- c("age", "bmi", "HbA1c_level", "blood_glucose_level")

# 2 x 2 
par(mfrow = c(2, 2))  

# boxplot
boxplot(diabetes_dataset$age, 
        main = "Boxplot of Age", 
        col = "lightblue", outline = TRUE)

boxplot(diabetes_dataset$bmi, 
        main = "Boxplot of BMI", 
        col = "lightblue", outline = TRUE)

boxplot(diabetes_dataset$HbA1c_level, 
        main = "Boxplot of HbA1c", 
        col = "lightblue", outline = TRUE)

boxplot(diabetes_dataset$blood_glucose_level, 
        main = "Boxplot of Glucose", 
        col = "lightblue", outline = TRUE)

par(mfrow = c(1, 1))


```
BMI has a large number of outliers, which should be retained or binned.
Blood Glucose Level has an upper bound outlier, but it is likely a true anomaly
HbA1c_level has a few outliers, but not too much
Age distribution is natural, no processing required

```{r chunk5}
# Distribution of age and BMI,HbA1c,Glucose
library(ggplot2)
ggplot(diabetes_dataset, aes(x = age, fill = as.factor(diabetes))) + 
  geom_histogram(binwidth = 5, position = "dodge") +
  labs(title = "Age Distribution by Diabetes Status", fill = "Diabetes")



```
```{r chunk5.1}
ggplot(diabetes_dataset, aes(x = 1:nrow(diabetes_dataset), y = bmi, color = as.factor(diabetes))) +
  geom_point(alpha = 0.4, size = 1) +
  labs(title = "BMI Distribution by Diabetes Status",
       x = "Index", y = "BMI", color = "Diabetes") +
  theme_minimal()
```
```{r chunk5.2}
ggplot(diabetes_dataset, aes(x = 1:nrow(diabetes_dataset), y = HbA1c_level, color = as.factor(diabetes))) +
  geom_point(alpha = 0.4, size = 1) +
  labs(title = "HbA1c Level by Diabetes Status",
       x = "Index", y = "HbA1c Level", color = "Diabetes") +
  theme_minimal()
```
```{r chunk5.3}
ggplot(diabetes_dataset, aes(x = 1:nrow(diabetes_dataset), y = blood_glucose_level, color = as.factor(diabetes))) +
  geom_point(alpha = 0.4, size = 1) +
  labs(title = "Blood Glucose Level by Diabetes Status",
       x = "Index", y = "Glucose Level", color = "Diabetes") +
  theme_minimal()
```
```{r chunk5.4}
ggplot(diabetes_dataset, aes(x = as.factor(hypertension), fill = as.factor(diabetes))) +
  geom_bar(position = "fill") +
  labs(title = "Diabetes Proportion by Hypertension",
       x = "Hypertension (0 = No, 1 = Yes)", y = "Proportion", fill = "Diabetes") +
  theme_minimal()
```

a Class Imbalance
- Observation from all plots: The number of diabetes-positive cases (diabetes = 1) is significantly lower than diabetes-negative cases.
- The dataset shows a strong class imbalance, roughly 1 diabetic case for every 10 non-diabetic cases.

b Age Distribution by Diabetes Status：

Most diabetes cases are concentrated in the 40+ age group. The number of diabetes cases increases steadily from age 40 to 60. This suggests that middle-aged and older adults have a significantly higher risk of diabetes.

c BMI Distribution by Diabetes Status：

Higher BMI is associated with more diabetes cases. There is a clear concentration of diabetic individuals in higher BMI ranges. Indicates that BMI is a strong risk factor for diabetes.

d HbA1c Level by Diabetes Status：

Individuals with HbA1c ≤ 5.0 are almost exclusively non-diabetic. The 6.0–6.5 range shows mixed cases, with relatively fewer diabetics. HbA1c ≥ 7.0 is almost entirely associated with diabetes.

This aligns well with clinical guidelines (HbA1c ≥ 6.5% indicates diabetes). bA1c is a highly predictive variable.

e Blood Glucose Level by Diabetes Status：

Glucose ≤ 125: almost no diabetic cases. Glucose 125–200: mixed, but majority are non-diabetic. Glucose ≥ 200: nearly all are diabetic cases.

This supports medical criteria that blood glucose over 200 mg/dL is a strong indicator of diabetes.

f Diabetes Proportion by Hypertension：

Diabetic proportion is much higher among individuals with hypertension. While only ~10% of non-hypertensive individuals have diabetes, over 25% of hypertensive individuals are diabetic.

Suggests a significant association or comorbidity between hypertension and diabetes.






```{r chunk6}
# Correlation Matrix Heatmap (numeric variables only)
library(corrplot)
corr_matrix <- cor(diabetes_dataset[, num_vars])
corrplot(corr_matrix, method = "color", type = "upper", tl.cex = 0.7)

```
Age and BMI have a weak to moderate positive correlation

Among other variables:

HbA1c_level and blood_glucose_level should be positively correlated in theory, but this data set has almost no linear correlation

Age is basically uncorrelated with HbA1c_level, blood_glucose_level, etc.



Feature Transformation and Scaling
```{r chunk7.0}
# Standardize numerical variables
# diabetes_dataset[num_vars] <- scale(diabetes_dataset[num_vars])

```

```{r chunk7}
# how gender affects diabetes
ggplot(diabetes_dataset, aes(x = gender, fill = as.factor(diabetes))) +
  geom_bar(position = "fill") +
  labs(title = "Diabetes Proportion by Gender", y = "Proportion")

```

There is little difference in the prevalence of diabetes between females and males, but the prevalence of diabetes in males is slightly higher. Others cannot reflect much information due to too little data.


```{r chunk8}
# BMI vs Age by diabetes status
ggplot(diabetes_dataset, aes(x = bmi, y = age, color = as.factor(diabetes))) +
  geom_point(alpha = 0.5) +
  labs(title = "BMI vs Age by Diabetes", color = "Diabetes")
```

People with diabetes are mainly concentrated in the middle-aged and elderly (over 40 years old) + high BMI area

When age and BMI increase together, there are more diabetic patients

```{r chunk9}
# ttset
t.test(age ~ diabetes, data = diabetes_dataset)

# chisquare
table_gender <- table(diabetes_dataset$gender, diabetes_dataset$diabetes)
chisq.test(table_gender)
```


There is a significant difference in age between the diabetes group (diabetes == 1) and the non-diabetes group (diabetes == 0)
The average age of patients with diabetes is about 20 years older than that of non-diabetes patients
There is a statistical association between gender and diabetes (may be affected by the other category and lead to inaccuracy)

```{r chunk10}
library(caret)
library(glmnet)
library(ggplot2)
library(dplyr)
library(e1071)
library(pROC)


# Remove original gender and smoking_history columns
cleaned_data <- diabetes_dataset %>%
  select(-gender, -smoking_history)

# Ensure target is a factor
cleaned_data$diabetes <- as.factor(cleaned_data$diabetes)
```

```{r chunk11}
# Split Data (70% Training, 30% Testing)
set.seed(42)
train_index <- createDataPartition(cleaned_data$diabetes, p = 0.7, list = FALSE)
train_data <- cleaned_data[train_index, ]
test_data  <- cleaned_data[-train_index, ]


```
Train GLM (Logistic Regression)
```{r chunk12}
glm_model <- glm(diabetes ~ ., data = train_data, family = "binomial",
                 control = glm.control(maxit = 100))


# Predict on test set
glm_pred_prob <- predict(glm_model, newdata = test_data, type = "response")
glm_pred <- ifelse(glm_pred_prob > 0.5, 1, 0)
glm_pred <- as.factor(glm_pred)

# Actual labels
actual <- test_data$diabetes

```
The model did not find the optimal solution within the maximum number of iterations. It is speculated that the variables Blood Glucose Level and HbA1c Level have too strong an impact on the predicted variables (causing the coefficient to increase infinitely) and the data separation is serious.

Evaluation for GLM
```{r chunk13}
# Confusion Matrix
conf_glm <- confusionMatrix(glm_pred, actual, positive = "1")
print(conf_glm)

# F1 Score
F1_Score <- function(pred, true, positive = "1") {
  cm <- table(pred, true)
  precision <- cm[positive, positive] / sum(cm[positive, ])
  recall <- cm[positive, positive] / sum(cm[, positive])
  f1 <- 2 * precision * recall / (precision + recall)
  return(f1)
}

f1_glm <- F1_Score(glm_pred, actual)
cat("F1 Score (GLM):", f1_glm, "\n")

```
The model’s overall prediction accuracy is 0.959, which is very high.
However, the recall for diabetic patients is relatively low at 0.6282.
Considering that the proportion of diabetic patients in the dataset is low,
and that Blood Glucose Level and HbA1c Level have an excessively strong influence,
this suggests that the model performs poorly in distinguishing between diabetic and non-diabetic individuals in certain ranges,


Lasso Logistic Regression + Visualization
```{r chunk14}
# Prepare matrices for glmnet
x_train <- model.matrix(diabetes ~ ., train_data)[, -1]
x_test  <- model.matrix(diabetes ~ ., test_data)[, -1]
y_train <- train_data$diabetes
y_test  <- test_data$diabetes

# Lasso logistic regression (alpha = 1 for L1)
set.seed(42)
lasso_model <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1, type.measure = "class")

# Best lambda
best_lambda <- lasso_model$lambda.min
cat("Best Lambda:", best_lambda, "\n")

# Predict on test data
lasso_pred_prob <- predict(lasso_model, newx = x_test, s = "lambda.min", type = "response")
lasso_pred <- ifelse(lasso_pred_prob > 0.5, 1, 0)
lasso_pred <- as.factor(lasso_pred)

# Confusion Matrix
conf_lasso <- confusionMatrix(lasso_pred, y_test, positive = "1")
print(conf_lasso)

# F1 Score
f1_lasso <- F1_Score(lasso_pred, y_test)
cat("F1 Score (Lasso):", f1_lasso, "\n")

```


Lasso is not significantly better than logistic regression (GLM). Although it does have some details, its overall performance is still limited by the structure of the data and the class imbalance problem.



```{r chunk15}
# Plot Lasso coefficient paths
plot(lasso_model$glmnet.fit, xvar = "lambda", label = TRUE)
title("Lasso Coefficients vs Log(Lambda)")

```
```{r chunk16}
roc_obj <- roc(as.numeric(y_test), as.numeric(lasso_pred_prob))
auc(roc_obj)
plot(roc_obj, main = "ROC Curve for Lasso")
```

```{r chunk17}
library(xgboost)

library(caret)
library(pROC)

```
```{r chunk18}
# Remove target column for X and convert to matrix
train_data$smoking_status_grouped <- NULL
test_data$smoking_status_grouped <- NULL
x_train <- train_data[, setdiff(names(train_data), "diabetes")]
x_train[] <- lapply(x_train, function(col) as.numeric(as.character(col)))
x_train <- as.matrix(x_train)

x_test <- test_data[, setdiff(names(test_data), "diabetes")]
x_test[] <- lapply(x_test, function(col) as.numeric(as.character(col)))
x_test <- as.matrix(x_test)

# convert label
y_train <- ifelse(train_data$diabetes == "1", 1, 0)
y_test <- ifelse(test_data$diabetes == "1", 1, 0)


```

```{r chunk18.1}
x_train_raw <- train_data[, setdiff(names(train_data), "diabetes")]
sapply(x_train_raw, function(col) sum(is.na(as.numeric(as.character(col)))))
sapply(train_data, class)

```

```{r chunk19}
# Define XGBoost parameters (binary:logistic for classification)
params <- list(
  objective = "binary:logistic",
  eval_metric = "logloss",
  max_depth = 6,
  eta = 0.1
)

# Convert to DMatrix
dtrain <- xgb.DMatrix(data = x_train, label = y_train)
dtest  <- xgb.DMatrix(data = x_test)

# Train model
set.seed(42)
xgb_model <- xgb.train(params = params, data = dtrain, nrounds = 100, verbose = 0)
```


```{r chunk20}
# Predict probabilities

xgb_pred_prob <- predict(xgb_model, dtest)

# Predict labels (using 0.5 threshold)
xgb_pred <- ifelse(xgb_pred_prob > 0.5, 1, 0)
xgb_pred <- as.factor(xgb_pred)
actual <- as.factor(y_test)
```





```{r chunk21}
# Confusion matrix
conf_xgb <- confusionMatrix(xgb_pred, actual, positive = "1")
print(conf_xgb)

# F1 Score
F1_Score <- function(pred, true, positive = "1") {
  cm <- table(pred, true)
  if (!positive %in% rownames(cm) || !positive %in% colnames(cm)) return(NA)
  precision <- cm[positive, positive] / sum(cm[positive, ])
  recall <- cm[positive, positive] / sum(cm[, positive])
  if (is.na(precision) || is.na(recall) || (precision + recall) == 0) return(0)
  f1 <- 2 * precision * recall / (precision + recall)
  return(f1)
}

f1_xgb <- F1_Score(xgb_pred, actual)
cat("F1 Score (XGBoost):", f1_xgb, "\n")

```



```{r chunk22}
roc_obj <- roc(y_test, xgb_pred_prob)
auc_val <- auc(roc_obj)
plot(roc_obj, main = paste("ROC Curve for XGBoost (AUC =", round(auc_val, 4), ")"))

```


XGBoost model performance is significantly better than the previous generalized linear model (GLM)

















